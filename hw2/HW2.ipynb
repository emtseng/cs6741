{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QgYJtiHHVomJ",
        "Ma2eM7vydn1l",
        "wTV628kEERgg",
        "LeGgwqV4Frjc",
        "lU6O3ljxEFWQ",
        "9CsPY00qViMX",
        "DD7egtitEyGe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgYJtiHHVomJ",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwfgo2gOU5oX",
        "colab_type": "code",
        "outputId": "8c2070e3-3a84-4d35-b342-486c3342f7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install -q torch torchtext opt_einsum\n",
        "!pip install -qU git+https://github.com/harvardnlp/namedtensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    17% |█████▌                          | 10kB 19.6MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 20kB 2.3MB/s eta 0:00:01\r\u001b[K    51% |████████████████▌               | 30kB 3.3MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 40kB 2.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 51kB 2.6MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for namedtensor (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofK7vC3wU-aV",
        "colab_type": "code",
        "outputId": "26cf75fa-def7-4d34-e789-0cbcae073cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "from torch.autograd import Variable\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from namedtensor import ntorch\n",
        "from google.colab import files\n",
        "from namedtensor.text import NamedField\n",
        "import numpy as np\n",
        "from torchtext.data.iterator import BPTTIterator\n",
        "from torchtext.data import Batch, Dataset\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files # only needed to save and upload files on google colab\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import heapq\n",
        "\n",
        "\n",
        "# Our input $x$\n",
        "TEXT = NamedField(names=(\"seqlen\",))\n",
        "\n",
        "# Fetch dataset\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/input.txt\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/train.5k.txt\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/train.txt\n",
        "!curl -qO https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW2/valid.txt\n",
        "    \n",
        "# Data distributed with the assignment\n",
        "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
        "    path=\".\", \n",
        "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
        "\n",
        "# Build vocab\n",
        "TEXT.build_vocab(train)\n",
        "\n",
        "class NamedBpttIterator(BPTTIterator):\n",
        "    def __iter__(self):\n",
        "        text = self.dataset[0].text\n",
        "        TEXT = self.dataset.fields['text']\n",
        "        TEXT.eos_token = None\n",
        "        text = text + ([TEXT.pad_token] * int(math.ceil(len(text) / self.batch_size)\n",
        "                                              * self.batch_size - len(text)))\n",
        "        data = TEXT.numericalize(\n",
        "            [text], device=self.device)\n",
        "        data = (data\n",
        "            .stack((\"seqlen\", \"batch\"), \"flat\")\n",
        "            .split(\"flat\", (\"batch\", \"seqlen\"), batch=self.batch_size)\n",
        "            .transpose(\"seqlen\", \"batch\")\n",
        "        )\n",
        "\n",
        "        dataset = Dataset(examples=self.dataset.examples, fields=[\n",
        "            ('text', TEXT), ('target', TEXT)])\n",
        "        while True:\n",
        "            for i in range(0, len(self) * self.bptt_len, self.bptt_len):\n",
        "                self.iterations += 1\n",
        "                seq_len = min(self.bptt_len, len(data) - i - 1)\n",
        "                yield Batch.fromvars(\n",
        "                    dataset, self.batch_size,\n",
        "                    text = data.narrow(\"seqlen\", i, seq_len),\n",
        "                    target = data.narrow(\"seqlen\", i+1, seq_len),\n",
        "                )\n",
        "                         \n",
        "            if not self.repeat:\n",
        "                return\n",
        "\n",
        "# create batches and iterators\n",
        "train_iter, val_iter, test_iter = NamedBpttIterator.splits(\n",
        "    (train, val, test), batch_size=10, device=torch.device(\"cuda\"), bptt_len=32, repeat=False)\n",
        "\n",
        "# global variable\n",
        "vocab_size = len(TEXT.vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  185k  100  185k    0     0  1626k      0 --:--:-- --:--:-- --:--:-- 1626k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  595k  100  595k    0     0  5003k      0 --:--:-- --:--:-- --:--:-- 5003k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 4982k  100 4982k    0     0  31.3M      0 --:--:-- --:--:-- --:--:-- 31.3M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  390k  100  390k    0     0  3648k      0 --:--:-- --:--:-- --:--:-- 3683k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTV628kEERgg",
        "colab_type": "text"
      },
      "source": [
        "## Assignment Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFboHTc2EWND",
        "colab_type": "text"
      },
      "source": [
        "In this homework you will be building several varieties of language models.\n",
        "\n",
        "We ask that you construct the following models in Torch:\n",
        "\n",
        "1. A count-based trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
        "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeGgwqV4Frjc",
        "colab_type": "text"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7MZHF9_FsM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repackage a hidden variable\n",
        "def forget_hist(h):\n",
        "    while not isinstance(h, Variable): \n",
        "        h = Variable(h.data)\n",
        "    return Variable(h.data)\n",
        "\n",
        "# Returns the perplexity given the cost\n",
        "def perplexity(cost):\n",
        "    return float('%.2f'%(np.exp(cost)))\n",
        "\n",
        "# Generates a plot of epochs vs perplexity per epoch\n",
        "def graph_perplexity(epoch_perplexity, model_name):\n",
        "    y = epoch_perplexity\n",
        "    x = range(len(epoch_perplexity))\n",
        "    print(\"min perplexity = \", np.min(epoch_perplexity))\n",
        "    plt.plot(x,y)\n",
        "    plt.title(model_name)\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()  \n",
        "    \n",
        "def download(file_name):\n",
        "    files.download(file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP4rL9s8EB5C",
        "colab_type": "text"
      },
      "source": [
        "## Model 1 - Count-based Trigram with Linear-Interpolation\n",
        "\n",
        "$$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkzMWfsliZHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addToAnyDict(anyDict, previousWordsTuple,w):\n",
        "    if previousWordsTuple not in anyDict:\n",
        "        anyDict[previousWordsTuple] = defaultdict(int)\n",
        "\n",
        "    anyDict[previousWordsTuple][w] +=1\n",
        "\n",
        "    # increment count for that word\n",
        "    anyDict[previousWordsTuple][-1] += 1\n",
        "    \n",
        "\n",
        "def getDicts(numDicts):\n",
        "    count = Counter()\n",
        "    dicts = []\n",
        "    dicts.append(Counter())\n",
        "    for k in range(1,numDicts):\n",
        "        dicts.append({})\n",
        "\n",
        "    totalNumWords = 0\n",
        "\n",
        "    for b in iter(train_iter):\n",
        "        for i in range(b.text.shape[\"batch\"]):\n",
        "            seq = b.text[{\"batch\": i}].tolist()\n",
        "\n",
        "            # Update each ngram dict\n",
        "            for z in range(1,numDicts):\n",
        "                for w in range(len(seq) - z):\n",
        "                    addToAnyDict(dicts[z], tuple([seq[w + j] for j in range(z)]),seq[w+z])\n",
        "\n",
        "            # Update total count\n",
        "            values = b.text.values.contiguous().view(-1).tolist()\n",
        "            dicts[0].update(values)\n",
        "            totalNumWords += len(values) \n",
        "  \n",
        "    dicts[0][-1] = totalNumWords\n",
        "    return dicts\n",
        "\n",
        "\n",
        "# Get prob of word given all the ngrams\n",
        "def getProbYtFromGrams(dicts, previousWordsTup, yt, alphas):\n",
        "    numDicts = len(previousWordsTup) + 1\n",
        "    unigramProb = dicts[0][yt] / dicts[0][-1]\n",
        "    gramProbs = [unigramProb]\n",
        "    \n",
        "    for z in range(1,numDicts):\n",
        "        myDict = dicts[z]\n",
        "        tupKey = previousWordsTup[numDicts - z - 1:]\n",
        "        if tupKey in myDict:\n",
        "            gramProb = myDict[tupKey][yt]/myDict[tupKey][-1] if myDict[tupKey][-1] != 0 else 0\n",
        "        else:\n",
        "            gramProb = 0\n",
        "        gramProbs.append(gramProb)\n",
        "    \n",
        "    return sum([gramProbs[i] * alphas[numDicts - i - 1] for i in range(numDicts)])\n",
        "\n",
        "\n",
        "# Predict next best numWords based on previousWord tuple\n",
        "def getNextWords(dicts, previousWordsTup, numWords, alphas):\n",
        "    lenVocab = len(TEXT.vocab)\n",
        "    probs = [(i, getProbYtFromGrams(dicts, previousWordsTup,i, alphas)) for i in range(lenVocab)]\n",
        "    nLargest = heapq.nlargest(numWords, probs, key= lambda x: x[1] )\n",
        "    return [TEXT.vocab.itos[x[0]] for x in nLargest]  \n",
        "\n",
        "# Predict probabilities for next word\n",
        "def getNextProbs(dicts, previousWordsTup, alphas):\n",
        "    lenVocab = len(TEXT.vocab)\n",
        "    probs = [(i, getProbYtFromGrams(dicts, previousWordsTup,i, alphas)) for i in range(lenVocab)]\n",
        "    return probs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdcWsjva1uRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trigram\n",
        "n = 3\n",
        "alphas = [.1,.8,.1]\n",
        "\n",
        "# 4-gram\n",
        "# n = 4\n",
        "# alphas = [.1,.1,.8,.1]\n",
        "\n",
        "dicts = getDicts(n)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_pZSXzoZhDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computes ngram perplexity\n",
        "\n",
        "numDictsMinusOne = n - 1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "totalTimes = 0\n",
        "totalLoss = 0\n",
        "for b in iter(val_iter):\n",
        "    for i in range(b.text.shape[\"batch\"]):\n",
        "        if totalTimes > 500:\n",
        "            break\n",
        "        seq = [i for i in b.text[{\"batch\": i}].tolist()]\n",
        "\n",
        "        for j in range(len(seq) - numDictsMinusOne):\n",
        "            nextProbs = getNextProbs(dicts, tuple([seq[j + k] for k in range(numDictsMinusOne)]),alphas)\n",
        "          \n",
        "            output = Variable(torch.FloatTensor([nextProbs])).log()\n",
        "            target = Variable(torch.LongTensor([seq[j+ n - 1]]))\n",
        "          \n",
        "\n",
        "            loss = criterion(output,target)\n",
        "            totalLoss += loss\n",
        "            totalTimes += 1\n",
        "            \n",
        "print(np.exp(totalLoss/totalTimes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU6O3ljxEFWQ",
        "colab_type": "text"
      },
      "source": [
        "## Model 2 - Neural Network Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB43d02wgg4F",
        "colab_type": "code",
        "outputId": "a33c8222-dea6-4340-8117-ee462cf5f50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1313
        }
      },
      "source": [
        "# Model 2 - A Neural Probabilistic Language Model\n",
        "class NPLM(nn.Module):\n",
        "    def __init__(self, h, m, n):\n",
        "        super(NPLM, self).__init__()\n",
        "        self.h = h\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.vocab_size = len(TEXT.vocab)\n",
        "        self.embedding = ntorch.nn.Embedding(self.vocab_size, self.m)\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        self.conv = ntorch.nn.Conv1d(self.m, self.h, self.n-1).spec(\"embedding\", \"seqlen\", \"state\") # input, time, output\n",
        "        self.fc = ntorch.nn.Linear(self.h, self.vocab_size).spec(\"state\", \"preds\")\n",
        "        \n",
        "    def forward(self, text):\n",
        "        padding = nn.ConstantPad2d((0,0,self.n-2,0), TEXT.vocab.stoi[\"<pad>\"])\n",
        "        text = ntorch.tensor(padding(text._tensor), names=(\"seqlen\", \"batch\"))\n",
        "        batch_embedding = self.embedding(text).transpose(\"batch\", \"embedding\", \"seqlen\")\n",
        "        batch_size = batch_embedding.shape[\"batch\"]\n",
        "        \n",
        "        x = self.conv(batch_embedding).tanh()\n",
        "        x = self.fc(x)\n",
        "        return x.transpose(\"seqlen\", \"batch\", \"preds\")\n",
        "        \n",
        "lr = 0.5\n",
        "\n",
        "nplm = NPLM(h=50, m=60, n=6).cuda()\n",
        "criterion = ntorch.nn.CrossEntropyLoss().spec(\"preds\")\n",
        "optimizer = optim.Adadelta(nplm.parameters(), lr=lr)\n",
        "\n",
        "epochs = 75\n",
        "\n",
        "for i in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_iter:\n",
        "        nplm.zero_grad()\n",
        "        preds = nplm(batch.text.cuda())\n",
        "        loss = criterion(preds.stack((\"seqlen\", \"batch\"), \"batch\"), batch.target.stack((\"seqlen\", \"batch\"), \"batch\"))\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"total loss\", total_loss.item()/len(train_iter))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss 6.404776920180723\n",
            "total loss 6.014144524526678\n",
            "total loss 5.899896460843373\n",
            "total loss 5.822559434165233\n",
            "total loss 5.764310590576592\n",
            "total loss 5.717507530120482\n",
            "total loss 5.678421498493976\n",
            "total loss 5.64483850580895\n",
            "total loss 5.615340267319277\n",
            "total loss 5.5889337483864026\n",
            "total loss 5.564986956755594\n",
            "total loss 5.543106914264199\n",
            "total loss 5.522956782487091\n",
            "total loss 5.504269309380379\n",
            "total loss 5.486844139952668\n",
            "total loss 5.470476549053356\n",
            "total loss 5.45496954335198\n",
            "total loss 5.440308667706541\n",
            "total loss 5.426422991071429\n",
            "total loss 5.413234859079174\n",
            "total loss 5.400747969556798\n",
            "total loss 5.388891055292599\n",
            "total loss 5.3776002447289155\n",
            "total loss 5.366818725796041\n",
            "total loss 5.3564900225903616\n",
            "total loss 5.34657110585198\n",
            "total loss 5.33703743545611\n",
            "total loss 5.3278456459767645\n",
            "total loss 5.318955397482788\n",
            "total loss 5.310372740963856\n",
            "total loss 5.302085910606713\n",
            "total loss 5.294073391781412\n",
            "total loss 5.286319720847676\n",
            "total loss 5.278820863812393\n",
            "total loss 5.271562701699656\n",
            "total loss 5.264505566910499\n",
            "total loss 5.257630970309811\n",
            "total loss 5.250926809918244\n",
            "total loss 5.244413928033563\n",
            "total loss 5.238086945998279\n",
            "total loss 5.231957293459552\n",
            "total loss 5.225988664479346\n",
            "total loss 5.220191144040448\n",
            "total loss 5.214544562177281\n",
            "total loss 5.209030765920827\n",
            "total loss 5.203663201914802\n",
            "total loss 5.198434474505164\n",
            "total loss 5.193323741394148\n",
            "total loss 5.188329321751291\n",
            "total loss 5.183462645223752\n",
            "total loss 5.17869379302926\n",
            "total loss 5.174030496987951\n",
            "total loss 5.169476118760757\n",
            "total loss 5.165012505378657\n",
            "total loss 5.160647724827883\n",
            "total loss 5.15638244944062\n",
            "total loss 5.152183062607573\n",
            "total loss 5.148052253657487\n",
            "total loss 5.14400010757315\n",
            "total loss 5.140039734832186\n",
            "total loss 5.136155335628227\n",
            "total loss 5.132346237629088\n",
            "total loss 5.128596977194492\n",
            "total loss 5.1249210009681585\n",
            "total loss 5.121301836811532\n",
            "total loss 5.117745871880379\n",
            "total loss 5.114262854991394\n",
            "total loss 5.1108279098537\n",
            "total loss 5.107446751290878\n",
            "total loss 5.104134842943202\n",
            "total loss 5.100880418997418\n",
            "total loss 5.097667343481067\n",
            "total loss 5.094500995051635\n",
            "total loss 5.091386752366609\n",
            "total loss 5.0883155389414805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CsPY00qViMX",
        "colab_type": "text"
      },
      "source": [
        "## Model 3 - LSTM Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tImRX2SlVtIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM Language Model\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # Global Hyperparameters\n",
        "        self.num_layers = 2\n",
        "        self.lr = 1\n",
        "        \n",
        "        # Hyperparameters - Zaremba (MEDIUM)\n",
        "        self.input_size = 650\n",
        "        param_init = 0.05\n",
        "        self.drop_prob = 0.50\n",
        "        self.grad_clip = 5\n",
        "        self.num_epochs = 39\n",
        "        self.lr_decay = 1.2\n",
        "        self.epoch_flat_lr = 6.0\n",
        "        \n",
        "        # Hyperparameters - Zaremba (LARGE)\n",
        "        # self.input_size = 1500\n",
        "        # param_init = 0.04\n",
        "        # self.drop_prob = 0.65\n",
        "        # grad_clip = 10\n",
        "        # self.num_epochs = 55\n",
        "        # self.lr_decay = 1.5\n",
        "        # self.epoch_flat_lr = 14.0\n",
        "\n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.input_size, num_layers=self.num_layers, dropout=self.drop_prob)\n",
        "        self.embeddings = nn.Embedding(vocab_size, self.input_size)\n",
        "        self.embeddings.weight.data.uniform_(-param_init, param_init)\n",
        "        self.lin_trans = nn.Linear(self.input_size,vocab_size)\n",
        "        self.lin_trans.weight.data.uniform_(-param_init, param_init)\n",
        "\n",
        "    def forward(self, inputs, h, c):\n",
        "        embeds = self.dropout(self.embeddings(inputs))\n",
        "        result, (h, c) = self.lstm(embeds, (h, c))        \n",
        "        result = self.dropout(result)\n",
        "        result = self.lin_trans(result.view(-1, self.input_size))\n",
        "        return result, (h, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO7xW9C1UJy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(lstm_model, data_iter, is_training):\n",
        "    \n",
        "    batch_size = vars(train_iter)['batch_size']\n",
        "    num_steps = vars(train_iter)['bptt_len']\n",
        "    \n",
        "    if is_training:\n",
        "        lstm_model.train()\n",
        "    else:\n",
        "        lstm_model.eval()\n",
        "        \n",
        "    # Init hidden and cell variables\n",
        "    h = Variable(torch.zeros(lstm_model.num_layers, batch_size, lstm_model.input_size)).cuda()\n",
        "    c = Variable(torch.zeros(lstm_model.num_layers, batch_size, lstm_model.input_size)).cuda()\n",
        "\n",
        "    costs = 0.0\n",
        "    for batch in data_iter:\n",
        "\n",
        "        # Padd text and target to seqlen 32\n",
        "        if batch.text.shape[\"seqlen\"] != num_steps:\n",
        "            pad_text = ntorch.ones((num_steps - batch.text.shape[\"seqlen\"],batch.text.shape[\"batch\"]), names=(\"seqlen\",\"batch\")).long().cuda()\n",
        "            batch.text = ntorch.cat((pad_text, batch.text), dim=\"seqlen\")\n",
        "            pad_target = ntorch.ones((num_steps - batch.target.shape[\"seqlen\"],batch.target.shape[\"batch\"]), names=(\"seqlen\",\"batch\")).long().cuda()\n",
        "            batch.target = ntorch.cat((pad_target, batch.target), dim=\"seqlen\")\n",
        "\n",
        "        lstm_model.zero_grad()\n",
        "        h, c = forget_hist(h), forget_hist(c)\n",
        "        inputs = Variable(batch.text.values.contiguous())\n",
        "        outputs, (h,c) = lstm_model(inputs, h, c)\n",
        "\n",
        "        # Format outputs/targets to the same size\n",
        "        outputs = outputs.view(num_steps, batch_size, vocab_size).view(-1, vocab_size)            \n",
        "        targets = torch.squeeze(Variable(batch.target.values.contiguous()).view(-1, batch_size * num_steps))\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        costs += loss.data.item()\n",
        "        \n",
        "        if is_training:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(lstm_model.parameters(), lstm_model.grad_clip)\n",
        "\n",
        "            for p in lstm_model.parameters():\n",
        "                p.data.add_(-lstm_model.lr, p.grad.data)\n",
        "\n",
        "    return perplexity(costs/len(data_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lARpH_VFZp9q",
        "colab_type": "code",
        "outputId": "48a3ea90-a70a-4fcf-e686-e3189526fbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1762
        }
      },
      "source": [
        "# Train model\n",
        "def train_lstm(lstm_model):\n",
        "    \n",
        "    print(\"Training...\")\n",
        "   \n",
        "    epochs_perplexity = []\n",
        "    for epoch in range(lstm_model.num_epochs):\n",
        "        \n",
        "        # Decay learning rate\n",
        "        if epoch > lstm_model.epoch_flat_lr:\n",
        "            lstm_model.lr = lstm_model.lr / lstm_model.lr_decay\n",
        "            \n",
        "        perp = run_epoch(lstm_model, train_iter, True)\n",
        "        epochs_perplexity.append(perp)\n",
        "        print('epoch:'+str(epoch+1)+'/'+str(lstm_model.num_epochs)+' perplexity:'+str(perp))\n",
        "        \n",
        "    print(\"Done Training\")\n",
        "    return lstm_model, epochs_perplexity\n",
        " \n",
        "\n",
        "# Initialize and Train LSTM model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lstm_model = LSTM().cuda()\n",
        "lstm_model, epoch_perplexity = train_lstm(lstm_model)\n",
        "\n",
        "# Epoch vs Perplexity visualization\n",
        "model_name = \"LSTM LM - Zaremba (Medium) Params\"\n",
        "graph_perplexity(epoch_perplexity, model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:1/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:2/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:3/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:4/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:5/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:6/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:7/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:8/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:9/39 perplexity:1.0\n",
            "shapes:\n",
            "torch.Size([320, 10001])\n",
            "torch.Size([320])\n",
            "epoch:10/39 perplexity:1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0544889c1ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Epoch vs Perplexity visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0544889c1ab8>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(lstm_model)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mperp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mepochs_perplexity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' perplexity:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ee3f13c1ad28>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lstm_model, data_iter, is_training)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Padd text and target to seqlen 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a0eaa721345d>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                               * self.batch_size - len(text)))\n\u001b[1;32m     48\u001b[0m         data = TEXT.numericalize(\n\u001b[0;32m---> 49\u001b[0;31m             [text], device=self.device)\n\u001b[0m\u001b[1;32m     50\u001b[0m         data = (data\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seqlen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/namedtensor/text/torch_text.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNamedField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGQH4eaWbUW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validate the trained LSTM\n",
        "def validate_lstm(lstm_model, num_steps, batch_size):\n",
        "    perp = run_epoch(lstm_model, val_iter, False)\n",
        "    print('LSTM Validation Perplexity:'+str(perp))\n",
        "    \n",
        "validate_lstm(lstm_model, 32, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li2-nwsTeMxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate and download Kaggle predictions\n",
        "def test_lstm(model, batch_text):\n",
        "    \n",
        "    indices = torch.tensor([10]).cuda()\n",
        "    predictions = []  \n",
        "    model.eval()\n",
        "\n",
        "    # Init hidden and cell\n",
        "    h = Variable(torch.zeros(model.num_layers, test_data.shape['batch'], model.input_size)).cuda()\n",
        "    c = Variable(torch.zeros(model.num_layers, test_data.shape['batch'], model.input_size)).cuda()\n",
        "    \n",
        "    inputs = Variable(batch_text.values.contiguous())\n",
        "    h, c = forget_hist(h), forget_hist(c)\n",
        "\n",
        "    outputs, (h,c) = model(inputs.t(), h, c)    \n",
        "    last_words = outputs.view(-1, 3165, 10001)\n",
        "    \n",
        "    # Fetch max 20 probs\n",
        "    for word_prob in last_words[-1]:\n",
        "        word_prob = np.array(word_prob.tolist())\n",
        "\n",
        "        # Remove (unk), (pad), and (eos)\n",
        "        word_prob[0] = 0\n",
        "        word_prob[1] = 0\n",
        "        word_prob[3] = 0\n",
        "        \n",
        "        word_ids = word_prob.argsort()[-20:][::-1]\n",
        "        entry_pred = ' '.join([TEXT.vocab.itos[i] for i in word_ids])\n",
        "        predictions.append(entry_pred)\n",
        "            \n",
        "    with open(\"predictions.txt\", \"w\") as fout:\n",
        "        print(\"id,word\", file=fout)\n",
        "        for i, l in enumerate(predictions, 1):\n",
        "            print(\"%d,%s\"%(i, l), file=fout)\n",
        "            \n",
        "    download('predictions.txt')\n",
        "\n",
        "# Generate/download Kaggle predictions\n",
        "test_lstm(lstm_model, test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}