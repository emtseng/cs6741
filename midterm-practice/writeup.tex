
\documentclass[11pt]{article}

\usepackage{common}
\title{Midterm Study Guide}
\author{Emily Tseng \\ et397@cornell.edu }
\begin{document}

\maketitle{}
\section{Introduction}

This is a study guide for the midterm exam in CS6741, spring 2020. It covers the following:

\begin{enumerate}
  \item Specification of generative models through *generative processes* and directed graphical models.
  \item Parameterization of distributions through features and *neural networks*. 
  \item Geometric representations of distributions through softmax and *simplex* representations. 
  \item Information-theoretic properties of discrete distributions, primarily *KL* but also entropy, cross-entropy. 
  \item Maximum-likelihood estimation (MLE) through *back-propagation* particularly chain-rule of log-softmax.
  \item Familiarity with basic neural network structures, in particular *attention*.
  \item Mastery of *Naive Bayes* and *Softmax regression* - parameterization, class-sizes, posterior inference, features, difference. 
  \item Comprehension of notation of *latent-variables* and their usage, including MLE in the presence of latent-variables.
  \item Understanding the *variational* formulation of the MLE objective in terms of ELBO and posterior gap. 
  \item Writing down the *EM* steps for clustering and understanding what each step is doing.
  \item Knowing the conditions under which EM is intractable and alternative variational approaches using simpler $q$.
  \item Using variational auto-encoders with neural $\rho$ as an alternative to EM.
  \item Conditions under which REINFORCE is used for backpropagation and the reasoning.
\end{enumerate}


\section{Generative models}

\section{Parameterization of distributions}

\subsection{Features}
\subsection{Neural networks}

\section{Geometric representations of distributions}

\subsection{Softmax representations}

\subsection{Simplex representations}

\section{Information-theoretic properties of discrete distributions}

\subsection{Entropy \& Perplexity}
\subsection{KL}
\subsection{Cross-entropy}

\section{MLE through backpropagation: chain-rule of log-softmax}

\section{Basic neural network structures, in particular attention}

\section{Naive Bayes and Softmax regression}

Parameterization, class sizes, posterior inference, features, difference.

\section{Latent variables}

\subsection{MLE in the presence of latent variables}

\section{Variational formulation of the MLE objective: ELBO, posterior gap}

\section{EM steps for clustering}

\subsection{Conditions under which EM is intractable}
\subsection{Alternative variational approaches using simpler q}

\section{Variational auto-encoders with neural $\rho$ as an alternative to EM}

\section{REINFORCE}

\subsection{Conditions under which REINFORCE is used for backpropagation}

\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
