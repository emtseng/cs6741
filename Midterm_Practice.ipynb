{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Midterm Practice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o64G03mkyEFG",
        "colab_type": "text"
      },
      "source": [
        "# Midterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMxUks3hyfjH",
        "colab_type": "text"
      },
      "source": [
        "## Technical Objectives \n",
        "\n",
        "The first half of the class had two high-level objectives: \n",
        "\n",
        "1) Internalize the formal language around probabilistic modeling in the context of deep learning for natural language processing. \n",
        "2) Get practice building high-performing neural network models for core classes of NLP models to prep for doing research. \n",
        "\n",
        "The homeworks have primarily focused on (2). The midterm will focus **exclusively** on (1). \n",
        "\n",
        "\n",
        "In particular we have covered the following technical topics, which are fair game for the midterm: \n",
        "\n",
        "* Specification of generative models through *generative processes* and directed graphical models.\n",
        "* Parameterization of distributions through features and *neural networks*. \n",
        "* Geometric representations of distributions through softmax and *simplex* representations. \n",
        "* Information-theoretic properties of discrete distributions, primarily *KL* but also entropy, cross-entropy. \n",
        "* Maximum-likelihood estimation (MLE) through *back-propagation* particularly chain-rule of log-softmax.\n",
        "* Familiarity with basic neural network structures, in particular *attention*.\n",
        "* Mastery of *Naive Bayes* and *Softmax regression* - paramaterization, class-sizes, posterior inference, features, difference. \n",
        "* Comprehension of notation of *latent-variables* and their usage, including MLE in the presence of latent-variables.\n",
        "* Understanding the *variational* formulation of the MLE objective in terms of ELBO and posterior gap. \n",
        "* Writing down the *EM* steps for clustering and understanding what each step is doing.\n",
        "* Knowing the conditions under which EM is intractable and alternative variational approaches using simpler $q$.\n",
        "* Using variational auto-encoders with neural $\\rho$ as an alternative to EM.\n",
        "* Conditions under which REINFORCE is used for backpropagation and the reasoning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbHjwBgTycvC",
        "colab_type": "text"
      },
      "source": [
        "# Midterm Practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkcer5MlyW-S",
        "colab_type": "text"
      },
      "source": [
        "The following is roughly the form that the midterm will take. Mastery of the following questions will give a good foundation for the midterm itself. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_CvaF2R_Xf3",
        "colab_type": "text"
      },
      "source": [
        "Consider the following natural language processing task classification task. You are given a sentence $x_1 \\ldots x_T$ from $|V|$ which we will assume is of a fixed length $T$. We want to classify this sentence into a sentiment class $y \\in \\{ \\text{positive, neutral, negative}\\}$. However we will assume that our sentences come from a broad set of different *domains* in particular $z \\in \\{ \\text{books, movies, music}\\}$. We are given a dataset with $x, y$ observed but with $z$ unobserved, however we believe it is important to model $z$ as part of a generative model of this data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-_PbjHp_l7B",
        "colab_type": "text"
      },
      "source": [
        "1) Specify this model as a naive generative process and a directed graphical model. Assume the simplest parameterization of the model where there is a single parameter for each probability. How many learned parameters (big-O) does this model have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuDeupcxBO2S",
        "colab_type": "text"
      },
      "source": [
        "2) In the parameterization for (1), there is an assumption that each word is generated in a completely different manner for each domain. Modify the above parameterization so that there is parameter sharing through *embeddings*. Argue that the model should take into account that \"good\" may be used similarly across different domain. How many learned parameters does this model have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac1s9YYHc7m",
        "colab_type": "text"
      },
      "source": [
        "2b) This model will likely utilize a softmax when generating words. What might be the advantages or disadvantages of instead specifying these distributions with an argmax or a sparse max?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reu3ZKomBvqd",
        "colab_type": "text"
      },
      "source": [
        "3) What is the MLE objective for fitting the learned parameters for (1) and (2)? Highlight which aspect of this objective makes this model difficult to fit in closed form. Write out the variational objective (ELBO + posterior gap) for this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_feM1raIFYKU",
        "colab_type": "text"
      },
      "source": [
        "4) The variational objective is a function of the model parameters $\\theta$ and the variational distribution $q$. Assume that we want to maximize ELBO for the $q$ term. Solve in closed-form for $q$ in this case. Argue (with a diagram) why this maximizes the ELBO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hpIwzM3F3RS",
        "colab_type": "text"
      },
      "source": [
        "5) Now assume that we want to maximize ELBO for the model parameters $\\theta$ for model (2). Write down the objective directly. How might we optimize this in pytorch? Give the computational complexity for computing this objective. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oTk06IuGioP",
        "colab_type": "text"
      },
      "source": [
        "6) What if instead of a 3 domains, the model had 100? How does run-time change? What approach might we take instead to speed up training? Specify the complete derivation and where any approximation occur. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jyEurn5HWV3",
        "colab_type": "text"
      },
      "source": [
        "7) Instead of EM, we now want to utilize a variational auto-encoder. Specify a form a VAE for this problem. Given the objective over the ELBO for both $\\theta$ and $q$. Draw a diagram of the gradients in terms of the variational parameters and specify where approximations occur. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilapb_maBMM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}